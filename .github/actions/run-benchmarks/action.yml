name: 'Run Benchmarks'
description: 'Execute performance benchmarks with pytest-benchmark'
inputs:
  benchmark-args:
    description: 'Additional arguments for pytest benchmark'
    required: false
    default: ''
  compare-with:
    description: 'Branch or commit to compare benchmarks with'
    required: false
    default: ''
  save-results:
    description: 'Save benchmark results for comparison'
    required: false
    default: 'true'

runs:
  using: 'composite'
  steps:
    - name: Create benchmarks directory
      run: mkdir -p .benchmarks
      shell: bash

    - name: Run performance benchmarks
      run: |
        echo "Running performance benchmarks..."
        uv run pytest \
          tests/ \
          -m benchmark \
          --benchmark-only \
          --benchmark-json=.benchmarks/benchmark_results.json \
          --benchmark-autosave \
          --benchmark-max-time=2 \
          --benchmark-min-rounds=5 \
          ${{ inputs.benchmark-args }} || true
      shell: bash

    - name: Compare with baseline
      if: inputs.compare-with != ''
      run: |
        echo "Comparing with baseline: ${{ inputs.compare-with }}"

        # Checkout baseline for comparison
        git fetch origin ${{ inputs.compare-with }}:baseline_branch || true
        git checkout baseline_branch -- . || true

        # Run baseline benchmarks
        uv run pytest \
          tests/ \
          -m benchmark \
          --benchmark-only \
          --benchmark-json=.benchmarks/baseline_results.json \
          --benchmark-max-time=2 \
          --benchmark-min-rounds=5 || true

        # Compare results
        if [ -f ".benchmarks/baseline_results.json" ] && [ -f ".benchmarks/benchmark_results.json" ]; then
          uv run pytest-benchmark compare \
            .benchmarks/baseline_results.json \
            .benchmarks/benchmark_results.json \
            --csv=.benchmarks/comparison.csv || true
        fi

        # Return to original branch
        git checkout - || true
      shell: bash

    - name: Generate benchmark report
      if: always()
      run: |
        echo "## Benchmark Results" >> benchmark-summary.md
        echo "" >> benchmark-summary.md

        if [ -f ".benchmarks/benchmark_results.json" ]; then
          echo "Performance benchmarks completed successfully." >> benchmark-summary.md
          echo "Results saved to: .benchmarks/benchmark_results.json" >> benchmark-summary.md

          # Extract key metrics from JSON if possible
          echo "" >> benchmark-summary.md
          echo "### Benchmark Results" >> benchmark-summary.md
          echo "See .benchmarks/benchmark_results.json for detailed metrics" >> benchmark-summary.md
        else
          echo "No benchmark results generated." >> benchmark-summary.md
        fi

        if [ -f ".benchmarks/comparison.csv" ]; then
          echo "" >> benchmark-summary.md
          echo "### Comparison with baseline" >> benchmark-summary.md
          echo "Comparison data saved to: .benchmarks/comparison.csv" >> benchmark-summary.md
        fi
      shell: bash

    - name: Upload benchmark results
      if: inputs.save-results == 'true' && always()
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: |
          .benchmarks/
          benchmark-summary.md
        retention-days: 30
        if-no-files-found: warn
